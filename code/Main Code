import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import cross_val_score, GridSearchCV

# Load data
train = pd.read_csv('C:/Users/Bryan/OneDrive/Desktop/train_house.csv')
test = pd.read_csv('C:/Users/Bryan/OneDrive/Desktop/test_house.csv')

# Separate target variable
y = train['SalePrice']
train.drop(['SalePrice'], axis=1, inplace=True)

# Combine train and test for uniform preprocessing
combined = pd.concat([train, test], keys=['train', 'test'])

# Drop columns with more than 50% missing values
missing_ratio = combined.isnull().sum() / combined.shape[0]
combined.drop(columns=missing_ratio[missing_ratio > 0.5].index, inplace=True)

# Impute 'LotFrontage' by Neighborhood median if present
if 'LotFrontage' in combined.columns and 'Neighborhood' in combined.columns:
    combined['LotFrontage'] = combined.groupby('Neighborhood')['LotFrontage'].transform(
        lambda x: x.fillna(x.median())
    )

# Impute remaining numerical features with median
num_cols = combined.select_dtypes(include=[np.number]).columns
for col in num_cols:
    if combined[col].isnull().sum() > 0:
        combined[col] = combined[col].fillna(combined[col].median())

# Impute categorical features with mode
cat_cols = combined.select_dtypes(include=['object']).columns
for col in cat_cols:
    if combined[col].isnull().sum() > 0:
        combined[col] = combined[col].fillna(combined[col].mode()[0])

# Feature engineering
combined['TotalSF'] = combined['GrLivArea'] + combined['TotalBsmtSF']
combined['Age'] = combined['YrSold'] - combined['YearBuilt']
combined['RemodelAge'] = combined['YrSold'] - combined['YearRemodAdd']

# One-hot encode categorical variables
combined = pd.get_dummies(combined)

# Split back into train and test
train_processed = combined.loc['train']
test_processed = combined.loc['test']

# Align test columns with train columns (fill missing with zeros)
test_processed = test_processed.reindex(columns=train_processed.columns, fill_value=0)

# Log-transform target to reduce skewness
y_log = np.log1p(y)

# Function to evaluate model using 5-fold CV RMSE
def evaluate_model(model, X, y):
    scores = cross_val_score(model, X, y, cv=5, scoring='neg_root_mean_squared_error')
    print(f"{model.__class__.__name__} RMSE: {-scores.mean():.4f}")

# Evaluate baseline models
print("Evaluating Linear Regression...")
linreg = LinearRegression()
evaluate_model(linreg, train_processed, y_log)

print("Evaluating Ridge Regression...")
ridge = Ridge(alpha=10)
evaluate_model(ridge, train_processed, y_log)

print("Evaluating default Gradient Boosting Regressor...")
gbm = GradientBoostingRegressor(random_state=42)
evaluate_model(gbm, train_processed, y_log)

# Hyperparameter tuning for Gradient Boosting Regressor
print("\nStarting hyperparameter tuning for Gradient Boosting Regressor...")

param_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.05, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'subsample': [0.8, 1.0]
}

grid_search = GridSearchCV(
    GradientBoostingRegressor(random_state=42),
    param_grid,
    cv=5,
    scoring='neg_root_mean_squared_error',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(train_processed, y_log)
print(f"Best params: {grid_search.best_params_}")
print(f"Best CV RMSE: {-grid_search.best_score_:.4f}")

# Train final model with best parameters
final_model = grid_search.best_estimator_
final_model.fit(train_processed, y_log)

# Predict on test data
preds_log = final_model.predict(test_processed)
preds = np.expm1(preds_log)  # inverse of log1p to get original price scale

# Ensure output folder exists
output_folder = 'predictionsubmission2'
os.makedirs(output_folder, exist_ok=True)

# Save submission
submission = pd.DataFrame({'Id': test['Id'], 'SalePrice': preds})
submission_path = os.path.join(output_folder, 'submission.csv')
submission.to_csv(submission_path, index=False)
print(f"Submission file saved as {submission_path}")

# ================================
# Feature Importance Visualization
# ================================

# Get feature importances from the final Gradient Boosting model
importances = final_model.feature_importances_
feature_names = train_processed.columns

# Create Series and sort
feature_importance_series = pd.Series(importances, index=feature_names)
top_features = feature_importance_series.sort_values(ascending=False).head(20)

# Plot top 20 important features
plt.figure(figsize=(12, 8))
sns.barplot(x=top_features.values, y=top_features.index, palette='viridis')
plt.title('Top 20 Most Important Features in Gradient Boosting Regressor')
plt.xlabel('Feature Importance Score')
plt.ylabel('Feature Name')
plt.tight_layout()

# Show and optionally save plot
plt.savefig(os.path.join(output_folder, 'feature_importance.png'))
plt.show()
